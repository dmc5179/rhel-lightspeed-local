---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: llamacpp-models
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: rhel-lightspeed
  name: rhel-lightspeed
spec:
  ports:
  - name: "8000"
    port: 8000
    targetPort: 8000
  - name: "8888"
    port: 8888
    targetPort: 8888
  selector:
    app: rhel-lightspeed
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: rhel-lightspeed
  name: rhel-lightspeed-deployment
spec:
  selector:
    matchLabels:
      app: rhel-lightspeed
  template:
    metadata:
      labels:
        app: rhel-lightspeed
      name: rhel-lightspeed
    spec:
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      initContainers:
        - name: copy-model-files-init
          image: registry.access.redhat.com/ubi9/ubi-minimal:latest
          command: ['sh', '-c', 'cp -r /tmp/initial-data/* /mnt/pvc/']
          volumeMounts:
            - name: llamacpp-models
              mountPath: /mnt/pvc
      # TODO: Test SCC and service account feature
      #serviceAccount: postgres-sa # TODO: Rename this since it covers all the containers here
      #automountServiceAccountToken: false
      containers:
      - name: pgvector
        env:
        - name: POSTGRES_PASSWORD
          value: postgres
        - name: POSTGRES_HOST
          value: 127.0.0.1
        - name: POSTGRES_PORT
          value: "5432"
        - name: PGVECTOR_DBNAME
          value: ragdb
        - name: POSTGRES_USER
          value: postgres
        image: registry.redhat.io/rhel-cla/rag-database-rhel10:latest
        ports:
        - containerPort: 8000
          name: porta
        - containerPort: 8888
          name: portb
# TODO: Make and SCC for the postgres container
# bash-5.2$ whoami
# postgres
# bash-5.2$ id
# uid=26(postgres) gid=26(postgres) groups=26(postgres)
#        securityContext:
#          seccompProfile:
#            type: RuntimeDefault
#          runAsNonRoot: true
#          allowPrivilegeEscalation: false
#          capabilities:
#            drop:
#              - ALL
#        #securityContext:
#          #seLinuxOptions:
#            #type: spc_t
#        volumeMounts:
#        - mountPath: /dev/shm
#          name: tmp-0
      - name: llamacpp-model
        command:
        - ramalama
        - --store
        - /models
        - --debug
        - serve
        - --port
        - "8888"
        - --host
        - 0.0.0.0
        - file:///models/Phi-4-mini-instruct-Q4_K_M.gguf
        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: HF_HOME
          value: /models/.cache/huggingface
        #image: quay.io/ramalama/cuda:latest # TODO: Had to side load the model from installer container and make my own image
        image: quay.io/danclark/ramalama/cuda:latest
        resources:
          limits:
            nvidia.com/gpu: 1
#        securityContext:
#          seccompProfile:
#            type: RuntimeDefault
#          runAsNonRoot: true
#          allowPrivilegeEscalation: false
#          capabilities:
#            drop:
#              - ALL
        #securityContext:
        #  seLinuxOptions:
        #    type: spc_t
        volumeMounts:
        #- mountPath: /dev/shm
        #  name: tmp-0
        #- mountPath: /models
        #  name: llamacpp-models-pvc
      - name: rlsapi
        env:
        - name: HF_HOME
          value: /tmp/huggingface
        - name: EMBED_CHUNK_OVERLAP
          value: "15"
        - name: LLM_TIMEOUT_SECONDS
          value: "120"
        - name: WORKFLOW_TIMEOUT_SECONDS
          value: "120"
        - name: ENABLE_AUTH
          value: "false"
        - name: DB_PASSWORD
          value: postgres
        - name: EMBED_CHUNK_SIZE
          value: "275"
        - name: OPENAI_MODEL
          value: /models/Phi-4-mini-instruct-Q4_K_M.gguf
        - name: OPENAI_API_BASE
          value: http://0.0.0.0:8888/v1
        - name: EMBED_MODEL
          value: ibm-granite/granite-embedding-30m-english
        - name: EMBED_DIMENSION
          value: "384"
        - name: REQUEST_TIMEOUT_ERROR
          value: "120"
        - name: EMBED_BATCH_SIZE
          value: "50"
        - name: LLM_PROVIDER
          value: openai
        image: registry.redhat.io/rhel-cla/rlsapi-rhel10:latest
#        securityContext:
#          seccompProfile:
#            type: RuntimeDefault
#          runAsNonRoot: true
#          allowPrivilegeEscalation: false
#          capabilities:
#            drop:
#              - ALL
        #securityContext:
        #  seLinuxOptions:
        #    type: spc_t
      volumes:
      - name: llamacpp-models-pvc
        persistentVolumeClaim:
          claimName: llamacpp-models
#      - emptyDir:
#          medium: Memory
#        name: tmp-0
